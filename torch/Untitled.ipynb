{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "excess-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "train = pd.read_csv('/home/yihang_toby/data/all_features.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "amber-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.read_csv('/home/yihang_toby/model/xgboost/feature_importance.csv',index_col= 0)\n",
    "feat_cols = list(importance.sort_values(by = '0',ascending = False)[:200].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "imported-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feat_cols = feat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "thick-police",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[feat_cols].mean().to_csv('f_mean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "inside-optics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yihang_toby/torch\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "approved-hazard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.504069666022587"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train['resp']>0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "matched-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,hidden_size,dropout_rate,activate):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_norm0 = nn.BatchNorm1d(len(all_feat_cols))\n",
    "        self.dropout0 = nn.Dropout(0.2)\n",
    "        self.dense1 = nn.Linear(len(all_feat_cols), hidden_size)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate/10)\n",
    "\n",
    "        self.dense2 = nn.Linear(hidden_size+len(all_feat_cols), hidden_size)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate/10)\n",
    "\n",
    "        self.dense3 = nn.Linear(hidden_size+hidden_size, hidden_size)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate/10)\n",
    "\n",
    "        self.dense4 = nn.Linear(hidden_size+hidden_size, hidden_size)\n",
    "        self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout4 = nn.Dropout(dropout_rate/10)\n",
    "\n",
    "        self.dense5 = nn.Linear(hidden_size+hidden_size, len(target_cols))\n",
    "\n",
    "        self.activate = activate\n",
    "        self.Relu = nn.ReLU(inplace=True)\n",
    "        self.PReLU = nn.PReLU()\n",
    "        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        # self.GeLU = nn.GELU()\n",
    "        self.RReLU = nn.RReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm0(x)\n",
    "        x = self.dropout0(x)\n",
    "\n",
    "        x1 = self.dense1(x)\n",
    "        x1 = self.batch_norm1(x1)\n",
    "        # x = F.relu(x)\n",
    "        # x = self.PReLU(x)\n",
    "        exec('x1 = self.'+self.activate+'(x1)')\n",
    "        x1 = self.dropout1(x1)\n",
    "\n",
    "        x = torch.cat([x, x1], 1)\n",
    "\n",
    "        x2 = self.dense2(x)\n",
    "        x2 = self.batch_norm2(x2)\n",
    "        # x = F.relu(x)\n",
    "        # x = self.PReLU(x)\n",
    "        exec('x2 = self.'+self.activate+'(x2)')\n",
    "        x2 = self.dropout2(x2)\n",
    "\n",
    "        x = torch.cat([x1, x2], 1)\n",
    "\n",
    "        x3 = self.dense3(x)\n",
    "        x3 = self.batch_norm3(x3)\n",
    "        # x = F.relu(x)\n",
    "        # x = self.PReLU(x)\n",
    "        exec('x3 = self.'+self.activate+'(x3)')\n",
    "        x3 = self.dropout3(x3)\n",
    "\n",
    "        x = torch.cat([x2, x3], 1)\n",
    "\n",
    "        x4 = self.dense4(x)\n",
    "        x4 = self.batch_norm4(x4)\n",
    "        # x = F.relu(x)\n",
    "        # x = self.PReLU(x)\n",
    "        exec('x4 = self.'+self.activate+'(x4)')\n",
    "        x4 = self.dropout4(x4)\n",
    "\n",
    "        x = torch.cat([x3, x4], 1)\n",
    "\n",
    "        x = self.dense5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "blond-corruption",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['resp','resp_1','resp_2','resp_3','resp_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "scenic-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketDataset:\n",
    "    def __init__(self, df):\n",
    "        self.features = df[all_feat_cols].values\n",
    "\n",
    "        self.label = df[target_cols].values.reshape(-1, len(target_cols))\n",
    "\n",
    "        self.weight = df['weight'].values.reshape(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'features': torch.tensor(self.features[idx], dtype=torch.float),\n",
    "            'label': torch.tensor(self.label[idx], dtype=torch.float),\n",
    "            'weight':torch.tensor(self.weight[idx],dtype = torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "guided-wings",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "for _fold in range(5):\n",
    "    device = 'cpu'\n",
    "    model = Model(400,0.2,'LeakyReLU')\n",
    "    model.to(device)\n",
    "    model_weights = f\"./train_newfeature_notweight{_fold}.pth\"\n",
    "    model.load_state_dict(torch.load(model_weights,map_location=torch.device('cpu')))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "confidential-dollar",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[feat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "modular-juice",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "instant-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "sapphire-trade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "\n",
    "    for data in dataloader:\n",
    "        features = data['features'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(features)\n",
    "\n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds).reshape(-1, len(target_cols))\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "twenty-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = np.zeros((len(train), len(target_cols)))\n",
    "test_set = MarketDataset(train)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=100)\n",
    "for _fold in range(2):\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device('cuda:0')\n",
    "    model = Model(400,0.2,'LeakyReLU')\n",
    "    model.to(device)\n",
    "    model_weights = f\"./train_newfeature_notweight_uscore{_fold}.pth\"\n",
    "    model.load_state_dict(torch.load(model_weights))\n",
    "\n",
    "    test_pred += inference_fn(model, test_loader, device) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "consolidated-glasgow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.52370986 0.52217388 0.52730095 0.52438268 0.52129334]\n",
      " [0.52370986 0.52217388 0.52730095 0.52438268 0.52129334]\n",
      " [0.52370986 0.52217388 0.52730095 0.52438268 0.52129334]\n",
      " ...\n",
      " [0.52370986 0.52217388 0.52730095 0.52438268 0.52129334]\n",
      " [0.52370986 0.52217388 0.52730095 0.52438268 0.52129334]\n",
      " [0.52370986 0.52217388 0.52730095 0.52438268 0.52129334]]\n",
      "[[0.53355592 0.48335709 0.47453764 0.510392   0.55275992]\n",
      " [0.53355592 0.48335709 0.47453764 0.510392   0.55275992]\n",
      " [0.53355592 0.48335709 0.47453764 0.510392   0.55275992]\n",
      " ...\n",
      " [0.53355592 0.48335709 0.47453764 0.510392   0.55275992]\n",
      " [0.53355592 0.48335709 0.47453764 0.510392   0.55275992]\n",
      " [0.53355592 0.48335709 0.47453764 0.510392   0.55275992]]\n",
      "[[0.53400351 0.50008081 0.50281575 0.51355971 0.54009853]\n",
      " [0.53400351 0.50008081 0.50281575 0.51355971 0.54009853]\n",
      " [0.53400351 0.50008081 0.50281575 0.51355971 0.54009853]\n",
      " ...\n",
      " [0.53400351 0.50008081 0.50281575 0.51355971 0.54009853]\n",
      " [0.53400351 0.50008081 0.50281575 0.51355971 0.54009853]\n",
      " [0.53400351 0.50008081 0.50281575 0.51355971 0.54009853]]\n",
      "[[0.49039418 0.44046317 0.42460583 0.44275691 0.50931792]\n",
      " [0.49039418 0.44046317 0.42460583 0.44275691 0.50931792]\n",
      " [0.49039418 0.44046317 0.42460583 0.44275691 0.50931792]\n",
      " ...\n",
      " [0.49039418 0.44046317 0.42460583 0.44275691 0.50931792]\n",
      " [0.49039418 0.44046317 0.42460583 0.44275691 0.50931792]\n",
      " [0.49039418 0.44046317 0.42460583 0.44275691 0.50931792]]\n",
      "[[0.49191923 0.44201237 0.42669339 0.4455682  0.51066166]\n",
      " [0.49191923 0.44201237 0.42669339 0.4455682  0.51066166]\n",
      " [0.49191923 0.44201237 0.42669339 0.4455682  0.51066166]\n",
      " ...\n",
      " [0.49191923 0.44201237 0.42669339 0.4455682  0.51066166]\n",
      " [0.49191923 0.44201237 0.42669339 0.4455682  0.51066166]\n",
      " [0.49191923 0.44201237 0.42669339 0.4455682  0.51066166]]\n",
      "[[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "[[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "[[0.51807308 0.481424   0.47038546 0.49600515 0.53482226]\n",
      " [0.51807308 0.481424   0.47038546 0.49600515 0.53482226]\n",
      " [0.51807308 0.481424   0.47038546 0.49600515 0.53482226]\n",
      " ...\n",
      " [0.51807308 0.481424   0.47038546 0.49600515 0.53482226]\n",
      " [0.51807308 0.481424   0.47038546 0.49600515 0.53482226]\n",
      " [0.51807308 0.481424   0.47038546 0.49600515 0.53482226]]\n",
      "[[0.56158814 0.52315703 0.53388378 0.54747206 0.56198189]\n",
      " [0.56158814 0.52315703 0.53388378 0.54747206 0.56198189]\n",
      " [0.56158814 0.52315703 0.53388378 0.54747206 0.56198189]\n",
      " ...\n",
      " [0.56158814 0.52315703 0.53388378 0.54747206 0.56198189]\n",
      " [0.56158814 0.52315703 0.53388378 0.54747206 0.56198189]\n",
      " [0.56158814 0.52315703 0.53388378 0.54747206 0.56198189]]\n",
      "[[0.48748432 0.49023059 0.49728285 0.4856213  0.4788778 ]\n",
      " [0.48748432 0.49023059 0.49728285 0.4856213  0.4788778 ]\n",
      " [0.48748432 0.49023059 0.49728285 0.4856213  0.4788778 ]\n",
      " ...\n",
      " [0.48748432 0.49023059 0.49728285 0.4856213  0.4788778 ]\n",
      " [0.48748432 0.49023059 0.49728285 0.4856213  0.4788778 ]\n",
      " [0.48748432 0.49023059 0.49728285 0.4856213  0.4788778 ]]\n",
      "[[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "[[0.44524238 0.47755174 0.46429652 0.44657525 0.44382145]\n",
      " [0.44524238 0.47755174 0.46429652 0.44657525 0.44382145]\n",
      " [0.44524238 0.47755174 0.46429652 0.44657525 0.44382145]\n",
      " ...\n",
      " [0.44524238 0.47755174 0.46429652 0.44657525 0.44382145]\n",
      " [0.44524238 0.47755174 0.46429652 0.44657525 0.44382145]\n",
      " [0.44524238 0.47755174 0.46429652 0.44657525 0.44382145]]\n",
      "[[0.81135958 0.53735465 0.56239666 0.69996077 0.82837346]\n",
      " [0.81135958 0.53735465 0.56239666 0.69996077 0.82837346]\n",
      " [0.81135958 0.53735465 0.56239666 0.69996077 0.82837346]\n",
      " ...\n",
      " [0.81135958 0.53735465 0.56239666 0.69996077 0.82837346]\n",
      " [0.81135958 0.53735465 0.56239666 0.69996077 0.82837346]\n",
      " [0.81135958 0.53735465 0.56239666 0.69996077 0.82837346]]\n",
      "[[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "[[0.47345375 0.4753923  0.47269657 0.48252574 0.47873408]\n",
      " [0.47345375 0.4753923  0.47269657 0.48252574 0.47873408]\n",
      " [0.47345375 0.4753923  0.47269657 0.48252574 0.47873408]\n",
      " ...\n",
      " [0.47345375 0.4753923  0.47269657 0.48252574 0.47873408]\n",
      " [0.47345375 0.4753923  0.47269657 0.48252574 0.47873408]\n",
      " [0.47345375 0.4753923  0.47269657 0.48252574 0.47873408]]\n",
      "[[0.57313323 0.52939677 0.54305485 0.5658485  0.5713993 ]\n",
      " [0.57313323 0.52939677 0.54305485 0.5658485  0.5713993 ]\n",
      " [0.57313323 0.52939677 0.54305485 0.5658485  0.5713993 ]\n",
      " ...\n",
      " [0.57313323 0.52939677 0.54305485 0.5658485  0.5713993 ]\n",
      " [0.57313323 0.52939677 0.54305485 0.5658485  0.5713993 ]\n",
      " [0.57313323 0.52939677 0.54305485 0.5658485  0.5713993 ]]\n",
      "[[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "[[0.51047482 0.51190233 0.51014608 0.52469796 0.51622885]\n",
      " [0.51047482 0.51190233 0.51014608 0.52469796 0.51622885]\n",
      " [0.51047482 0.51190233 0.51014608 0.52469796 0.51622885]\n",
      " ...\n",
      " [0.51047482 0.51190233 0.51014608 0.52469796 0.51622885]\n",
      " [0.51047482 0.51190233 0.51014608 0.52469796 0.51622885]\n",
      " [0.51047482 0.51190233 0.51014608 0.52469796 0.51622885]]\n",
      "[[0.50762248 0.48691548 0.49058001 0.50757849 0.51207253]\n",
      " [0.50762248 0.48691548 0.49058001 0.50757849 0.51207253]\n",
      " [0.50762248 0.48691548 0.49058001 0.50757849 0.51207253]\n",
      " ...\n",
      " [0.50762248 0.48691548 0.49058001 0.50757849 0.51207253]\n",
      " [0.50762248 0.48691548 0.49058001 0.50757849 0.51207253]\n",
      " [0.50762248 0.48691548 0.49058001 0.50757849 0.51207253]]\n",
      "[[0.3884017  0.42657867 0.4106489  0.37293942 0.38452242]\n",
      " [0.3884017  0.42657867 0.4106489  0.37293942 0.38452242]\n",
      " [0.3884017  0.42657867 0.4106489  0.37293942 0.38452242]\n",
      " ...\n",
      " [0.3884017  0.42657867 0.4106489  0.37293942 0.38452242]\n",
      " [0.3884017  0.42657867 0.4106489  0.37293942 0.38452242]\n",
      " [0.3884017  0.42657867 0.4106489  0.37293942 0.38452242]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(480,500):\n",
    "    test_set = MarketDataset(train[i:i+1])\n",
    "    test_loader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=100)\n",
    "    test_pred = np.zeros((len(train), len(target_cols)))\n",
    "    for _fold in range(2): \n",
    "        torch.cuda.empty_cache()\n",
    "        device = torch.device('cuda:0')\n",
    "        model = Model(400,0.2,'LeakyReLU')\n",
    "        model.to(device)\n",
    "        model_weights = f\"./train_newfeature_notweight_uscore{_fold}.pth\"\n",
    "        model.load_state_dict(torch.load(model_weights))\n",
    "\n",
    "        test_pred += inference_fn(model, test_loader, device) / 2\n",
    "    print(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "numerical-filter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52370983, 0.52217382, 0.52730095, 0.52438268, 0.52129334],\n",
       "       [0.5335559 , 0.48335706, 0.4745376 , 0.51039194, 0.55275992],\n",
       "       [0.53400354, 0.50008081, 0.50281577, 0.51355971, 0.54009859],\n",
       "       [0.4903944 , 0.44046307, 0.42460571, 0.44275691, 0.50931814],\n",
       "       [0.49191935, 0.44201224, 0.42669328, 0.44556814, 0.51066181],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.51807314, 0.481424  , 0.47038546, 0.49600521, 0.53482234],\n",
       "       [0.56158814, 0.52315703, 0.53388378, 0.54747206, 0.56198189],\n",
       "       [0.48748422, 0.49023055, 0.49728276, 0.48562118, 0.47887769],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.4452424 , 0.47755174, 0.46429648, 0.44657519, 0.44382142],\n",
       "       [0.8113597 , 0.53735451, 0.56239656, 0.69996083, 0.82837367],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.47345378, 0.47539227, 0.47269654, 0.48252572, 0.47873408],\n",
       "       [0.57313326, 0.52939674, 0.54305485, 0.5658485 , 0.57139933],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.51047482, 0.51190233, 0.51014605, 0.52469799, 0.51622885],\n",
       "       [0.50762248, 0.48691545, 0.49058001, 0.50757849, 0.51207253],\n",
       "       [0.38840163, 0.42657867, 0.41064891, 0.37293938, 0.38452238]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred[480:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "valuable-underground",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.54541683, 0.55348092, 0.5545046 , 0.55175203, 0.5441066 ],\n",
       "       [0.4979822 , 0.5106159 , 0.51587005, 0.52036893, 0.50469224],\n",
       "       [0.49839479, 0.48261628, 0.48523717, 0.49193117, 0.49934795],\n",
       "       [0.52310771, 0.51354869, 0.51695497, 0.53717345, 0.53275695],\n",
       "       [0.47960562, 0.54963052, 0.54858252, 0.51314647, 0.47189623],\n",
       "       [0.53914431, 0.5918535 , 0.58822444, 0.55441801, 0.52688372],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.49519993, 0.51342468, 0.51373142, 0.50352187, 0.49299106],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.50866961, 0.51458235, 0.51505435, 0.49805598, 0.50115015],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.50694911, 0.52725282, 0.5257116 , 0.52597809, 0.50927612],\n",
       "       [0.49051458, 0.46891861, 0.46849638, 0.4773028 , 0.49531654],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.24045364, 0.39535849, 0.36019941, 0.25977237, 0.23618499],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.46715018, 0.53589624, 0.54355371, 0.51739322, 0.45318794],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.70940702, 0.50123636, 0.53275119, 0.67667513, 0.70756093],\n",
       "       [0.49945146, 0.49632013, 0.49910022, 0.50236484, 0.50151485],\n",
       "       [0.50287184, 0.48517802, 0.49282956, 0.49786693, 0.50335503],\n",
       "       [0.47156559, 0.48529097, 0.48798394, 0.47976309, 0.47110189],\n",
       "       [0.51053825, 0.51866874, 0.52098396, 0.52209353, 0.51010962],\n",
       "       [0.51258165, 0.51536044, 0.51536581, 0.50546269, 0.50903994],\n",
       "       [0.5187988 , 0.49065273, 0.49660592, 0.51845241, 0.52594891],\n",
       "       [0.71673736, 0.59864229, 0.62722969, 0.69973707, 0.72097303],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.50491467, 0.54400745, 0.53934465, 0.51574799, 0.50176454],\n",
       "       [0.62432063, 0.5851672 , 0.59610096, 0.61769035, 0.62486947],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.48713103, 0.49373986, 0.49234636, 0.51186892, 0.50513959],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.47206546, 0.4721224 , 0.47470272, 0.47344097, 0.47361735],\n",
       "       [0.48297459, 0.47507894, 0.4794773 , 0.48286164, 0.48428455],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.50995889, 0.49479608, 0.50210872, 0.50153299, 0.50390086],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.50829105, 0.49853751, 0.50102381, 0.49752815, 0.50606798],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.44013609, 0.43359868, 0.43122905, 0.44181953, 0.44890621],\n",
       "       [0.52185303, 0.53046626, 0.5389069 , 0.53454104, 0.51920843],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.78756121, 0.56114084, 0.5949437 , 0.72300109, 0.80613381],\n",
       "       [0.32842916, 0.42914352, 0.40040892, 0.33970252, 0.32039091],\n",
       "       [0.48308347, 0.52514252, 0.52253392, 0.50265938, 0.47599073],\n",
       "       [0.50319499, 0.50074159, 0.50320411, 0.50168572, 0.50187558],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.43027414, 0.49977043, 0.49001826, 0.4469433 , 0.42403027],\n",
       "       [0.48607683, 0.46715456, 0.47118637, 0.48270606, 0.48929547],\n",
       "       [0.48468944, 0.48602532, 0.48952815, 0.48929749, 0.48579437],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.49313208, 0.50341739, 0.50714195, 0.49864879, 0.49413371],\n",
       "       [0.48212767, 0.47066082, 0.46427235, 0.47252432, 0.48748147],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.51418343, 0.51238488, 0.51703253, 0.52149117, 0.51628631],\n",
       "       [0.49851008, 0.50103307, 0.50362232, 0.5109749 , 0.5006672 ],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.62073115, 0.57324485, 0.57756089, 0.60886869, 0.62871671],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.47468069, 0.50884347, 0.50896865, 0.48145148, 0.46658923],\n",
       "       [0.31722788, 0.4255341 , 0.40428212, 0.35130642, 0.32120368],\n",
       "       [0.51884753, 0.52067587, 0.5217666 , 0.52628839, 0.52069467],\n",
       "       [0.49649684, 0.50732802, 0.50586976, 0.50483155, 0.49814972],\n",
       "       [0.4626471 , 0.48198521, 0.48016842, 0.47244783, 0.46436027],\n",
       "       [0.52464423, 0.51628399, 0.52037463, 0.5179259 , 0.52233878],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.51456195, 0.5317705 , 0.53039309, 0.51585129, 0.50984854],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.50501201, 0.52000594, 0.52235889, 0.51208447, 0.50272653],\n",
       "       [0.58972321, 0.52357472, 0.54131994, 0.61575432, 0.61201993],\n",
       "       [0.44533153, 0.48859464, 0.47590777, 0.43555976, 0.42915897],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.51519507, 0.51144177, 0.50919604, 0.5125291 , 0.51510242],\n",
       "       [0.5150353 , 0.54728159, 0.54276681, 0.52246851, 0.51114935],\n",
       "       [0.49216682, 0.48186623, 0.48766641, 0.49400452, 0.49396586],\n",
       "       [0.52561662, 0.48658688, 0.49648702, 0.51871997, 0.52984655],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.73475528, 0.60507923, 0.67125002, 0.73014198, 0.73342496],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.49677987, 0.49399583, 0.49802181, 0.49027906, 0.49297279],\n",
       "       [0.53985971, 0.53503424, 0.53715476, 0.56484163, 0.54805803],\n",
       "       [0.40273285, 0.47606857, 0.46267174, 0.39260346, 0.38830598],\n",
       "       [0.50389946, 0.51510324, 0.51434417, 0.50890632, 0.50259522],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.51652631, 0.535043  , 0.53297895, 0.5194107 , 0.51305458],\n",
       "       [       nan,        nan,        nan,        nan,        nan],\n",
       "       [0.35761048, 0.44804396, 0.43521185, 0.37075618, 0.35356306],\n",
       "       [0.52257112, 0.53337514, 0.53098321, 0.5253365 , 0.5201118 ]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_score = roc_auc_score(test[target_cols].values, test_pred)\n",
    "logloss_score = log_loss(test[target_cols].values, test_pred,sample_weight = test['weight'].values)\n",
    "\n",
    "test_pred = np.median(test_pred, axis=1)\n",
    "test_pred = np.where(test_pred >= 0.5, 1, 0).astype(int)\n",
    "test_score = utility_score_bincount(date=test.date.values, weight=test.weight.values, resp=test.resp.values,\n",
    "                                     action=test_pred)\n",
    "print(f'{NFOLDS} models test score: {test_score}\\tauc_score: {auc_score:.4f}\\tlogloss_score:{logloss_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "sensitive-copper",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-a08e79f1749e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/yihang_toby/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-cf87c2af02e8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_norm0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(len(X)):\n",
    "    x = np.array(X.iloc[i:i+1])\n",
    "    X_input = torch.from_numpy(x).to(device).double()\n",
    "    preds = []\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        model.double()\n",
    "        preds.append(model(X_input).detach().numpy().reshape(1,-1))\n",
    "    preds = np.concatenate(preds,axis=0)\n",
    "    preds = np.mean(preds,axis=0)\n",
    "    test_pred = np.median(preds)\n",
    "    actions.append(int(test_pred>0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-badge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "satisfied-sustainability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "compliant-request",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feature_39*abs(feature_113)', 'feature_63+feature_120/abs(feature_63)', 'feature_44/abs(feature_41)', 'feature_43/abs(feature_45)', 'feature_44+feature_45*abs(feature_41)', 'feature_41+feature_43*abs(feature_43)', 'feature_45+feature_63/abs(feature_41)', 'feature_31*abs(feature_113)', 'feature_45+feature_63', 'feature_63+feature_120*abs(feature_44)']\n",
      "    feature_39*abs(feature_113)  feature_63+feature_120/abs(feature_63)  \\\n",
      "0                     -2.117449                                     NaN   \n",
      "1                      1.453864                                     NaN   \n",
      "2                      3.064786                                     NaN   \n",
      "3                      1.680562                                     NaN   \n",
      "4                     -0.689900                                     NaN   \n",
      "..                          ...                                     ...   \n",
      "95                    -1.979598                                     NaN   \n",
      "96                    -0.798290                                     NaN   \n",
      "97                    -0.080724                                     NaN   \n",
      "98                    -0.822628                                     NaN   \n",
      "99                    -3.059544                                0.586655   \n",
      "\n",
      "    feature_44/abs(feature_41)  feature_43/abs(feature_45)  \\\n",
      "0                    -0.094122                    0.791564   \n",
      "1                    -0.199594                    1.111741   \n",
      "2                     4.160914                    9.764341   \n",
      "3                     0.316290                    2.663376   \n",
      "4                    -0.081805                    0.597743   \n",
      "..                         ...                         ...   \n",
      "95                   -0.811949                    2.170739   \n",
      "96                   -6.652858                    4.041699   \n",
      "97                   -6.652858                    4.041699   \n",
      "98                   -6.652858                    4.041699   \n",
      "99                    0.222005                   80.612397   \n",
      "\n",
      "    feature_44+feature_45*abs(feature_41)  \\\n",
      "0                                4.750237   \n",
      "1                               -2.418824   \n",
      "2                                0.015612   \n",
      "3                               -0.261304   \n",
      "4                                1.754509   \n",
      "..                                    ...   \n",
      "95                              -0.735913   \n",
      "96                              -1.018315   \n",
      "97                              -1.018315   \n",
      "98                              -1.018315   \n",
      "99                               0.021217   \n",
      "\n",
      "    feature_41+feature_43*abs(feature_43)  \\\n",
      "0                                6.456169   \n",
      "1                                5.185503   \n",
      "2                                5.309806   \n",
      "3                                1.738504   \n",
      "4                                2.640546   \n",
      "..                                    ...   \n",
      "95                               7.645971   \n",
      "96                               8.085638   \n",
      "97                               8.085638   \n",
      "98                               8.085638   \n",
      "99                              21.942001   \n",
      "\n",
      "    feature_45+feature_63/abs(feature_41)  feature_31*abs(feature_113)  \\\n",
      "0                               -0.134494                          NaN   \n",
      "1                               -0.130065                          NaN   \n",
      "2                              -34.787650                          NaN   \n",
      "3                               -3.543492                          NaN   \n",
      "4                                0.103839                          NaN   \n",
      "..                                    ...                          ...   \n",
      "95                              -0.325001                    -8.757717   \n",
      "96                              -6.445135                     5.288051   \n",
      "97                              -5.031925                    -0.634300   \n",
      "98                              -5.031845                    -4.400303   \n",
      "99                              -7.466348                    -6.006001   \n",
      "\n",
      "    feature_45+feature_63  feature_63+feature_120*abs(feature_44)  \n",
      "0               -0.469558                                     NaN  \n",
      "1               -0.742930                                     NaN  \n",
      "2               -1.356599                                     NaN  \n",
      "3               -2.153737                                     NaN  \n",
      "4                0.090428                                     NaN  \n",
      "..                    ...                                     ...  \n",
      "95              -0.550824                                     NaN  \n",
      "96              -2.185612                                     NaN  \n",
      "97              -1.706378                                     NaN  \n",
      "98              -1.706351                                     NaN  \n",
      "99              -3.555044                                0.216666  \n",
      "\n",
      "[100 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "print(feat_cols[:10])\n",
    "print(train[feat_cols[:10]].iloc[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-michael",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
